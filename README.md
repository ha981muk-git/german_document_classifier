# **ğŸ“˜ German Document Classifier**


This document provides a structure for developers working on the **German Document Classifier**.

*   **Included:** Installation steps, project structure, and usage instructions.
*   **Deep Dive:** For architecture and technical details, please see the [Technical System Documentation](./APPENDIX.md).


---
# **0.0 ğŸš€ Getting Started**



# **1.0 Running in Google Colab / Kaggle**

### **Clone the Repository**

Run the following commands in a Colab/Kaggle cell to clone the project and navigate into the directory:

```bash
!git clone https://github.com/ha981muk-git/german_document_classifier.git
%cd german_document_classifier
```

### **Install Dependencies**

```bash
!pip install uv
!uv pip install --system -r pyproject.toml
```



### **Fine-Tune the BERT Model**

Execute the main script to start the fine-tuning process:

```bash
!python -m app.main --train
```

# **2.0 ğŸ› ï¸ Installation ( Local Development)**
## **Prerequisites**

### **Before Cloning the Repository**

Ensure you have **uv** installed.
```bash
# MacOS/Linux
curl -LsSf https://astral.sh/uv/install.sh | sh

# Windows
powershell -c "irm https://astral.sh/uv/install.ps1 | iex"
```
---
## **2.1 Clone the repository**

```bash
git clone https://github.com/ha981muk-git/german_document_classifier.git
cd german_document_classifier
```

## **2.2 Create virtual environment**

```bash
uv sync

# MacOS/Linux
source .venv/bin/activate

# Windows
.venv\Scripts\activate
```
---


### **2.3 Synthetic Data Generation (Optional)**

```bash
python -m  app.main --generate
```

### **2.4 Prepare CSV File  For Datasets Training (Optional)**

```bash
python -m app.main --prepare
```

## **2.5 Training the BERT Models**

```bash
python -m app.main --train
```

## **2.6 Alternatively Generate, Prepare and Training the BERT Models (All At Once)**

```bash
python -m app.main --all
```

## **2.7 FastAPI Web Server**

The FastAPI service wraps the trained `DocumentClassifier` and exposes a single `/predict` endpoint that powers both the web UI and any programmatic client. It accepts either a `text` form field (for raw strings) or a `file` upload (for PDFs, images, or DOCs) and routes the request to the right inference path. Because the server also mounts the static frontend under `/`, you only need one process to serve both the UI and the API.

Start the server :
```bash
uvicorn app.api.api:app --reload --port 8080
```

Send freeâ€‘form text for classification:
```bash
curl -X POST http://127.0.0.1:8080/predict \
     -F "model_name=bert-base-german-cased" \
     -F "text=Dies ist eine deutsche Beispielrechnung."
```

Send pdf for classification:
```bash
curl -X POST http://127.0.0.1:8080/predict \
     -F "model_name=bert-base-german-cased" \
     -F "file=@app/data/raw/contracts/01_Vertrag.pdf"
```
### Open the UI:
ğŸ‘‰ http://localhost:8080

## **2.8 ğŸ³ Running with Docker (Alternative)**

For easier dependency management and deployment, you can build and run the entire application using Docker. This is the recommended way to run the service in production. But you need to train and get the model first for testing the model.

### **Build the Docker Image**

```bash
docker build -t german-document-classifier .
```

### **Run the Docker Container**
```bash
docker run -p 8080:8080 german-document-classifier
```
### Allows:

1. **âœ” Uploading PDFs, images:** `classifier.predict_file` extracts text via OCR/loader logic before inference.
2. **âœ” Text classification:** Directly send German text via the form field or curl request.
3. **âœ” Real-time inference:** The model is loaded once at startup, keeping latency low for repeated predictions.



### **2.9 Hyperparameter Searching**

```bash
python -m app.hyperparamsearch
```

## **3. ğŸ“ Project Structure**

```text
german_document_classifier/
â”‚
â”œâ”€â”€ config.yaml
â”œâ”€â”€ environment.yaml
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ optuna_studies.db
â”‚
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ flow.py
â”‚   â”œâ”€â”€ hyperparamsearch.py
â”‚   â”‚
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â””â”€â”€ api.py
â”‚   â”‚
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ evaluate.py
â”‚   â”‚   â”œâ”€â”€ paths.py
â”‚   â”‚   â”œâ”€â”€ prepare_data.py
â”‚   â”‚   â”œâ”€â”€ predict.py
â”‚   â”‚   â””â”€â”€ train.py
â”‚   â”‚
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â””â”€â”€ (data files not shown)
â”‚   â”‚
â”‚   â”œâ”€â”€ sampler/
â”‚   â”‚   â”œâ”€â”€ doc_generator.py
â”‚   â”‚   â””â”€â”€ make_synthetic_data.py
â”‚   â”‚
â”‚   â”œâ”€â”€ static/
â”‚   â”‚   â”œâ”€â”€ index.html
â”‚   â”‚   â””â”€â”€ style.css
â”‚   â”‚
â”‚   â””â”€â”€ notebooks/
â”‚       â”œâ”€â”€ 01_data_exploration.ipynb
â”‚       â”œâ”€â”€ 02_model_training.ipynb
â”‚       â”œâ”€â”€ 03_evaluation.ipynb
â”‚       â”œâ”€â”€ 04_data_extraction.ipynb
â”‚       â”œâ”€â”€ colab.ipynb
â”‚       â””â”€â”€ kaggle.ipynb
â”‚
â””â”€â”€ README.md

```
