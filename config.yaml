# Configuration for the training pipeline.

label_map:
  complaints: "complaint"
  contracts: "contract"
  invoices: "invoice"
  orders: "order"
  paymentreminders: "reminder"

models_to_train:
  # --- High Performance (Standard) ---
  # "The Optimized One": Best general performance, Whole Word Masking. ~440MB
  - "deepset/gbert-base"

  # "The Classic Baseline": The standard German BERT by MDZ. Very common. ~440MB
  - "dbmdz/bert-base-german-cased"

  # "Legacy/Generic": Often refers to the older Deepset model. ~440MB
  - "bert-base-german-cased"

  # --- Architecturally Different ---
  # "The Efficient One": Uses ELECTRA (Discriminator/Generator). Good efficiency. ~440MB
  - "deepset/gelectra-base"

  # "The Fast One": Distilled model. Smallest (~270MB), fastest speed, lower costs.
  - "distilbert-base-german-cased"

  # "The Big Data One": RoBERTa architecture. Trained on massive OSCAR dataset. ~500MB
  - "uklfr/gottbert-base"

training:
  learning_rate: 3.0e-5
  epochs: 10

synthetic_data:
  per_category_v0: 200
  per_category_v1: 100
  overwrite: false

data_split:
  # e.g., 0.3 creates a 70% train / 30% temp split
  validation_test_split_size: 0.3
  # e.g., 0.5 splits the 30% temp set into 15% validation and 15% test
  test_proportion_of_split: 0.5