# Configuration for the training pipeline.

label_map:
  complaints: "complaint"
  contracts: "contract"
  invoices: "invoice"
  orders: "order"
  paymentreminders: "reminder"

models_to_train:
  # --- High Performance (Standard) ---
  # "The Optimized One": Best general performance, Whole Word Masking. ~440MB
  #- "deepset/gbert-base"

  # "The Classic Baseline": The standard German BERT by MDZ. Very common. ~440MB
  #- "dbmdz/bert-base-german-cased"

  # "Legacy/Generic": Often refers to the older Deepset model. ~440MB
  - "bert-base-german-cased"

  # --- Architecturally Different ---
  # "The Efficient One": Uses ELECTRA (Discriminator/Generator). Good efficiency. ~440MB
  #- "deepset/gelectra-base"

  # "The Fast One": Distilled model. Smallest (~270MB), fastest speed, lower costs.
  - "distilbert-base-german-cased"

  # "The Big Data One": RoBERTa architecture. Trained on massive OSCAR dataset. ~500MB
  #- "uklfr/gottbert-base"

training:
  learning_rate: 3.0e-5
  epochs: 1

synthetic_data:
  per_category_v0: 200
  per_category_v1: 100
  overwrite: false

data_split:
  # e.g., 0.3 creates a 70% train / 30% temp split
  validation_test_split_size: 0.4
  # e.g., 0.5 splits the 30% temp set into 15% validation and 15% test
  test_proportion_of_split: 0.5


#  (existing configurations)

hpo_config:
  n_trials: 10
  epochs: 3
  keep_top_n_trials: 2
  storage_db: "sqlite:///optuna_studies.db"
  leaderboard_path: "./models/hpo_leaderboard.csv"
  search_space:
    learning_rate:
      type: "float"
      args: [1.0e-6, 5.0e-5, "log"]
    dropout:
      type: "float"
      args: [0.0, 0.3]
    weight_decay:
      type: "float"
      args: [0.0, 0.3]
    batch_size:
      type: "categorical"
      args: [[2, 4, 8]]
